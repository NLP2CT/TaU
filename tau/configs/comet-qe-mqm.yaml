mtme_data: /home/nlp2ct01/runzhe/.mt-metrics-eval/mt-metrics-eval-v2/
save_dir: ./comet-qe-mqm-results
# Highly recommend exploring the learning rate setting on a held-out set.
# lr is set to 1e-4 for all the models in our experiment
lr: 1e-4
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0
mcd_runs: 30
optimize_paras: ln

model: wmt21-comet-qe-mqm

data:
  wmt21.news:
    en-de:
      batch: 16
      steps: 5
    en-ru:
      batch: 32
      steps: 2
    zh-en:
      batch: 32
      steps: 5

  wmt21.tedtalks:
    en-de:
      batch: 32
      steps: 1
    en-ru:
      batch: 32
      steps: 2
    zh-en:
      batch: 16
      steps: 3


